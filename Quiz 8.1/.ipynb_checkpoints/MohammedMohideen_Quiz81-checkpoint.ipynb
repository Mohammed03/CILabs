{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TgcpRfdgwwF"
   },
   "source": [
    "# Lecture 8.1: Introduction to Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu6Bc_iRgwwG"
   },
   "source": [
    "## Chapter Outline\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F88OE_DbgwwG"
   },
   "source": [
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Chapter-Learning-Objectives\" data-toc-modified-id=\"Chapter-Learning-Objectives-2\">Chapter Learning Objectives</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-3\">Imports</a></span></li><li><span><a href=\"#1.-Convolutional-Neural-Networks-(CNNs)\" data-toc-modified-id=\"1.-Convolutional-Neural-Networks-(CNNs)-4\">1. Convolutional Neural Networks (CNNs)</a></span></li><li><span><a href=\"#2.-Cooking-up-a-CNN\" data-toc-modified-id=\"2.-Cooking-up-a-CNN-5\">2. Cooking up a CNN</a></span></li><li><span><a href=\"#3.-The-CNN-Recipe-Book\" data-toc-modified-id=\"3.-The-CNN-Recipe-Book-6\">3. The CNN Recipe Book</a></span></li><li><span><a href=\"#4.-CNN-vs-Fully-Connected-NN\" data-toc-modified-id=\"4.-CNN-vs-Fully-Connected-NN-7\">4. CNN vs Fully Connected NN</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PnImeq6gwwG"
   },
   "source": [
    "## Chapter Learning Objectives\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUoSi-CWgwwG"
   },
   "source": [
    "- Describe the terms convolution, kernel/filter, pooling, and flattening\n",
    "- Explain how convolutional neural networks (CNNs) work\n",
    "- Calculate the number of parameters in a given CNN architecture\n",
    "- Create a CNN in `PyTorch`\n",
    "- Discuss the key differences between CNNs and fully connected NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "te3qPoWJgwwH"
   },
   "source": [
    "## Imports\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykinE7x-gwwH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'font.size': 16, 'axes.labelweight': 'bold', 'axes.grid': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZAwKBY5jyz3"
   },
   "outputs": [],
   "source": [
    "def plot_conv(image, filter):\n",
    "    \"\"\"Plot convs with matplotlib.\"\"\"\n",
    "    d = filter.shape[-1]\n",
    "    conv = torch.nn.Conv2d(1, 1, kernel_size=(d, d), padding=1)\n",
    "    with torch.no_grad():\n",
    "        conv.weight[:] = filter\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(8, 4), ncols=2)\n",
    "    ax1.imshow(image, cmap='gray')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(\"Original\")\n",
    "    ax2.imshow(conv(image[None, None, :]).detach().squeeze(), cmap='gray')\n",
    "    ax2.set_title(\"Filtered\")\n",
    "    ax2.axis('off')\n",
    "    plt.tight_layout();\n",
    "\n",
    "def plot_convs(image, conv_layer, axis=False):\n",
    "    \"\"\"Plot convs with matplotlib. Sorry for this lazy code :D\"\"\"\n",
    "    filtered_image = conv_layer(image[None, None, :])\n",
    "    n = filtered_image.shape[1]\n",
    "    if n == 1:\n",
    "        fig, (ax1, ax2) = plt.subplots(figsize=(8, 4), ncols=2)\n",
    "        ax1.imshow(image, cmap='gray')\n",
    "        ax1.set_title(\"Original\")\n",
    "        ax2.imshow(filtered_image.detach().squeeze(), cmap='gray')\n",
    "        ax2.set_title(\"Filter 1\")\n",
    "        ax1.grid(False)\n",
    "        ax2.grid(False)\n",
    "        if not axis:\n",
    "            ax1.axis(False)\n",
    "            ax2.axis(False)\n",
    "        plt.tight_layout();\n",
    "    elif n == 2:\n",
    "        filtered_image_1 = filtered_image[:,0,:,:]\n",
    "        filtered_image_2 = filtered_image[:,1,:,:]\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(figsize=(10, 4), ncols=3)\n",
    "        ax1.imshow(image, cmap='gray')\n",
    "        ax1.set_title(\"Original\")\n",
    "        ax2.imshow(filtered_image_1.detach().squeeze(), cmap='gray')\n",
    "        ax2.set_title(\"Filter 1\")\n",
    "        ax3.imshow(filtered_image_2.detach().squeeze(), cmap='gray')\n",
    "        ax3.set_title(\"Filter 2\")\n",
    "        ax1.grid(False)\n",
    "        ax2.grid(False)\n",
    "        ax3.grid(False)\n",
    "        if not axis:\n",
    "            ax1.axis(False)\n",
    "            ax2.axis(False)\n",
    "            ax3.axis(False)\n",
    "        plt.tight_layout();\n",
    "    elif n == 3:\n",
    "        filtered_image_1 = filtered_image[:,0,:,:]\n",
    "        filtered_image_2 = filtered_image[:,1,:,:]\n",
    "        filtered_image_3 = filtered_image[:,2,:,:]\n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(figsize=(12, 4), ncols=4)\n",
    "        ax1.imshow(image, cmap='gray')\n",
    "        ax1.set_title(\"Original\")\n",
    "        ax2.imshow(filtered_image_1.detach().squeeze(), cmap='gray')\n",
    "        ax2.set_title(\"Filter 1\")\n",
    "        ax3.imshow(filtered_image_2.detach().squeeze(), cmap='gray')\n",
    "        ax3.set_title(\"Filter 2\")\n",
    "        ax4.imshow(filtered_image_3.detach().squeeze(), cmap='gray')\n",
    "        ax4.set_title(\"Filter 3\")\n",
    "        ax1.grid(False)\n",
    "        ax2.grid(False)\n",
    "        ax3.grid(False)\n",
    "        ax4.grid(False)\n",
    "        if not axis:\n",
    "            ax1.axis(False)\n",
    "            ax2.axis(False)\n",
    "            ax3.axis(False)\n",
    "            ax4.axis(False)\n",
    "        plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmqiQHgCgwwI"
   },
   "source": [
    "## 1. Convolutional Neural Networks (CNNs)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YexcxMgogwwI"
   },
   "source": [
    "### 1.1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1Yt1T9dgwwI"
   },
   "source": [
    "We've been dealing with \"fully connected neural networks\" meaning that every neuron in a given layer is connected to every neuron in the next layer. This has two key implications:\n",
    "1. It results in a lot of parameters.\n",
    "2. The order of our features doesn't matter.\n",
    "\n",
    "Consider the simple image and fully connected network below:\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/cnn-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eim4agrNgwwI"
   },
   "source": [
    "Every input node is connected to every node in the next layer - is that really necessary? When you look at this image, how do you know that it's Tom?\n",
    "- You notice the structure in the image (there's a face, shoulders, a smile, etc.)\n",
    "- You notice how different structures are positioned and related (the face is on top of the shoulders, etc.)\n",
    "- You probably use the shading (colour) to infer things about the image too but we'll talk more about that later.\n",
    "\n",
    "The point here is that **the structure of our data (the pixels) is important**. So maybe, we should have each hidden node only look at a small area of the image, like this:\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/cnn-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmkbHcBugwwI"
   },
   "source": [
    "We have far fewer parameters now because we're acknowledging that pixels that are far apart are probably not all that related and so don't need to be connected. We're seeing that structure is important here, so then why should we need to flatten this image at all? Let's be crazy and not flatten the image, but instead, make our hidden layer a 2D matrix:\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/cnn-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx5C2xcPgwwI"
   },
   "source": [
    "We're almost there!\n",
    "\n",
    "As it stands, each group of 2 x 2 pixels has 4 unique weights associated with it (one for each pixel), which are being summed up into a single value in the hidden layer. But we don't need the weights to be different for each group, we're looking for **structure**, we don't care if Tom's face is in the top left or the bottom right, we're just looking for a face!\n",
    "\n",
    "Let's summarise the weights into a weight \"filter\":\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/cnn-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zR7PA5ShgwwI"
   },
   "source": [
    "- Let's see how the filter works\n",
    "- We'll display some arbitrary values for our pixels\n",
    "- The filter \"convolves\" over each group of pixels, multiplies corresponding elements and sums them up to give the values in the output nodes:\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/cnn-5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvpW3EIRgwwI"
   },
   "source": [
    "As we'll see, we can add as many of these \"filters\" as we like to make more complex models that can identify more useful things:\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/cnn-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQu_rxIYgwwI"
   },
   "source": [
    "We just made a **convolutional neural network** (CNN). Instead of fully-connected hidden nodes, we have 2D filters that we \"convolve\" over our input data. This has two key advantages:\n",
    "1. We have less parameters than a fully connected network.\n",
    "2. We preserve the useful structure of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSNuc4YBgwwI"
   },
   "source": [
    "### 1.2. Convolutions and Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbZST9AxgwwI"
   },
   "source": [
    "Convolution really just means \"to pass over the data\". What are we \"passing\"? Our filters - which are also called **kernels**. Here's another gif like the one we saw earlier:\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/conv-1.gif)\n",
    "\n",
    ">Source: modified after [theano-pymc.readthedocs.io](https://theano-pymc.readthedocs.io/en/latest/tutorial/conv_arithmetic.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVnlTvJWgwwI"
   },
   "source": [
    "So how does this help us extract structure from the data? Well let's see some examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "evry1zDlgwwI",
    "outputId": "d8e6da74-3f59-4a3c-ce5d-0de579d8deda"
   },
   "outputs": [],
   "source": [
    "image = torch.from_numpy(plt.imread(\"tom_bw.png\"))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlR09wq-gwwJ"
   },
   "source": [
    "We can blur this image by applying a filter with the following weights:\n",
    "\n",
    "$$\\begin{bmatrix} 0.0625 & 0.125 & 0.0625 \\\\ 0.125 & 0.25 & 0.125 \\\\ 0.0625 & 0.125 & 0.0625 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeEIVI85gwwJ"
   },
   "outputs": [],
   "source": [
    "kernel = torch.tensor([[[[ 0.0625,  0.1250,  0.0625],\n",
    "                         [ 0.1250,  0.2500,  0.1250],\n",
    "                         [ 0.0625,  0.1250,  0.0625]]]])\n",
    "plot_conv(image, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PsVqANUgwwJ"
   },
   "source": [
    "How about this one:\n",
    "\n",
    "$$\\begin{bmatrix} -2 & -1 & 0 \\\\ -1 & 1 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7Nu-9Q2gwwJ"
   },
   "outputs": [],
   "source": [
    "kernel = torch.tensor([[[[ -2,  -1,  0],\n",
    "                         [ -1,   1,  1],\n",
    "                         [  0,   1,  2]]]])\n",
    "plot_conv(image, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ut0GOzafgwwJ"
   },
   "source": [
    "One more:\n",
    "\n",
    "$$\\begin{bmatrix} -1 & -1 & -1 \\\\ -1 & 8 & -1 \\\\ -1 & -1 & -1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLm5V4WFgwwJ"
   },
   "outputs": [],
   "source": [
    "kernel = torch.tensor([[[[  -1,  -1,   -1],\n",
    "                         [  -1,   8,   -1],\n",
    "                         [  -1,  -1,   -1]]]])\n",
    "plot_conv(image, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIIgJ1UPgwwJ"
   },
   "source": [
    "[Here's a great website](https://setosa.io/ev/image-kernels/) where you can play around with other filters. We usually use **odd numbers for filters** so that they are applied symmetrically around our input data. Did you notice in the gif earlier that the output from applying our kernel was smaller than the input? Take a look again:\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/conv-1.gif)\n",
    "\n",
    ">Source: modified after [theano-pymc.readthedocs.io](https://theano-pymc.readthedocs.io/en/latest/tutorial/conv_arithmetic.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9rDvcN3gwwJ"
   },
   "source": [
    "By default, our kernels are only applied where the filter fully fits on top of the input. But we can control this behaviour and the size of our output with:\n",
    "- `padding`: \"pads\" the outside of the input 0's to allow the kernel to reach the boundary pixels\n",
    "- `strides`: controls how far the kernel \"steps\" over pixels.\n",
    "\n",
    "Below is an example with:\n",
    "- `padding=1`: we have `1` layer of 0's around our border\n",
    "- `strides=(2,2)`: our kernel moves 2 data points to the right for each row, then moves 2 data points down to the next row\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/conv-2.gif)\n",
    "\n",
    ">Source: modified after [theano-pymc.readthedocs.io](https://theano-pymc.readthedocs.io/en/latest/tutorial/conv_arithmetic.html).\n",
    "\n",
    "We'll look more at these below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HsLdf4igwwJ"
   },
   "source": [
    "## 2. Cooking up a CNN\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haxKicffgwwJ"
   },
   "source": [
    "### 2.1. Ingredient 1: Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGBho2ZfgwwJ"
   },
   "source": [
    "We showed some example kernels above. In CNNs the actual values in **the kernels are the weights your network will learn during training**: your network will learn what structures are important for prediction.\n",
    "\n",
    "In PyTorch, convolutional layers are defined as `torch.nn.Conv2d`, there are 5 important arguments we need to know:\n",
    "1. `in_channels`: how many features are we passing in. Our features are our colour bands, in greyscale, we have 1 feature, in colour, we have 3 channels.\n",
    "2. `out_channels`: how many kernels do we want to use. Analogous to the number of hidden nodes in a hidden layer of a fully connected network.\n",
    "3. `kernel_size`: the size of the kernel. Above we were using 3x3. Common sizes are 3x3, 5x5, 7x7.\n",
    "4. `stride`: the \"step-size\" of the kernel.\n",
    "5. `padding`: the number of pixels we should pad to the outside of the image so we can get edge pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYLbWS-PgwwJ"
   },
   "outputs": [],
   "source": [
    "# 1 kernel of (3,3)\n",
    "\n",
    "conv_layer = torch.nn.Conv2d(1, 1, kernel_size=(3, 3))\n",
    "plot_convs(image, conv_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3f_Czd9gwwJ"
   },
   "outputs": [],
   "source": [
    "# 2 kernels of (3,3)\n",
    "\n",
    "# Try to add your conv_layer with kernel_size=(3, 3), input size as 1, output as 2\n",
    "\n",
    "conv_layer_2=#your code\n",
    "\n",
    "plot_convs(image, conv_layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLbKUC6cgwwJ"
   },
   "outputs": [],
   "source": [
    "# 3 kernels of (5,5)\n",
    "\n",
    "# Try to add your conv_layer with kernel_size=(5, 5), input size as 1, output as 3\n",
    "\n",
    "conv_layer=#your code\n",
    "\n",
    "plot_convs(image, conv_layer_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KeUUiyMgwwJ"
   },
   "source": [
    "If we use a kernel with no padding, our output image will be smaller as we noted earlier. Let's demonstrate that by using a larger kernel now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ck2q6XJ_gwwJ"
   },
   "outputs": [],
   "source": [
    "# 1 kernel of (51,51)\n",
    "\n",
    "# Try to add your conv_layer with kernel_size=(5, 5), input size as 1, output as 1\n",
    "\n",
    "conv_layer_51=#your code\n",
    "\n",
    "plot_convs(image, conv_layer_51, axis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdbUCPbbgwwK"
   },
   "source": [
    "As we saw, we can add `padding` to the outside of the image to avoid this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-M-q6X2gwwK"
   },
   "outputs": [],
   "source": [
    "# 1 kernel of (51,51) with padding\n",
    "conv_layer = torch.nn.Conv2d(1, 1, kernel_size=(51, 51), padding=25)\n",
    "plot_convs(image, conv_layer, axis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "st9QANzZgwwK"
   },
   "source": [
    "> Setting `padding = kernel_size // 2` will always result in an output the same shape as the input. Think about why this is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJsGzKECgwwL"
   },
   "source": [
    "Finally, we also saw before how `strides` influence the size of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pr3ReQ0cgwwL"
   },
   "outputs": [],
   "source": [
    "# 1 kernel of (25,25) with stride of 3\n",
    "\n",
    "# Try to add your conv_layer with kernel_size=(25, 25), input size as 1, output as 1 and stride=3, no padding\n",
    "\n",
    "conv_layer_s3 = #your code\n",
    "plot_convs(image, conv_layer_s3, axis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iS5NUVFBgwwL"
   },
   "source": [
    "With CNN we are no longer flattening our data, so what are our \"features\"?\n",
    "Our features are called \"channels\" in CNN-lingo -> they are like the colour channels in an image:\n",
    "- A grayscale image has 1 feature/channel\n",
    "- A coloured image has 3 features/channel\n",
    "    \n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/channels-1.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/channels-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzWgrSidgwwL"
   },
   "source": [
    "What's important with CNNs is that the **size of our input data does not impact how many parameters we have in our convolutonal layers**. For example, your kernels don't care how big your image is (i.e., 28 x 28 or 256 x 256), all that matters is:\n",
    "1. How many features (\"channels\") you have: `in_channels`\n",
    "2. How many filters you use in each layer: `out_channels`\n",
    "3. How big the filters are: `kernel_size`\n",
    "\n",
    "Let's see some diagrams:\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/cnn-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyzU5W6HgwwL"
   },
   "source": [
    "For coloured images (3 channels):\n",
    "\n",
    "![](img/cnn-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ox0CsfUUgwwM"
   },
   "source": [
    "### 2.2. Ingredient 2: Flattening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0_dPVZsgwwM"
   },
   "source": [
    "With our brand new, shiny convolutional layers, we're basically just passing images through the network - cool!\n",
    "\n",
    "But we're going to eventually want to do some regression or classification. That means that by the end of our network, we are going to need to `torch.nn.Flatten()` our images:\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/cnn-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY5sJCgKgwwM"
   },
   "source": [
    "Let's make that simple CNN above in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqoXqZzHgwwM"
   },
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = torch.nn.Sequential(\n",
    "            #    - in_channels=1: Input has 1 channel (e.g., grayscale images).\n",
    "            #    - out_channels=3: Outputs 3 feature maps.\n",
    "            #    - kernel_size=(3, 3): 3x3 convolution filter.\n",
    "            #    - padding=1: Adds 1 pixel of zero-padding on all sides to preserve spatial dimensions.\n",
    "\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(3, 3), padding=1),\n",
    "            torch.nn.ReLU(),  #    - ReLU replaces negative values with 0: f(x) = max(0, x).\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(3, 3), padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(20000, 1) #    - Input size: 20000 (flattened size of the feature maps) 2*100*100.\n",
    "#    - Output size: 1 (final prediction).\n",
    "#    - Used for binary classification, producing a single logit output.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PJ9DL6dgwwM"
   },
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "summary(model, (1, 100, 100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Rs1l_a8gwwM"
   },
   "source": [
    "Oh man! 20,000 parameters in that last layer, geez. Is there a way we can reduce this somehow? Glad you asked! See you in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please explain why the size of the above input is (1, 100, 100) and what each parameter represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your explanation\n",
    "\n",
    "Explanation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICHvngx3gwwM"
   },
   "source": [
    "### 2.3. Ingredient 3: Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeoqAliygwwM"
   },
   "source": [
    "Pooling is how we can reduce the number of parameters we get out of a `torch.nn.Flatten()`. It's pretty simple, we just aggregate the data, usually using the maximum or average of a window of pixels. Here's an example of max pooling:\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/pool.gif)\n",
    "\n",
    "> Source: modified after [www.oreilly.com/](https://www.oreilly.com/radar/visualizing-convolutional-neural-networks/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S549AabgwwM"
   },
   "source": [
    "We use \"pooling layers\" to reduce the shape of our image as it's passing through the network. So when we eventually `torch.nn.Flatten()`, we'll have less features in that flattened layer! We can implement pooling with `torch.nn.MaxPool2d()`. Let's try it out and reduce the number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPYDhtecgwwM"
   },
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(3, 3), padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2, 2)),\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(3, 3), padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2, 2)),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(1250, 1) #2*25*25\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCiEzgO_gwwM"
   },
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "summary(model, (1, 100, 100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxfcZgVCgwwM"
   },
   "source": [
    "We reduced that last layer to 1,251 parameters. Nice job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QZHv9StgwwM"
   },
   "source": [
    "## 3. The CNN Recipe Book\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKV0bq7OgwwM"
   },
   "source": [
    "Here's a CNN diagram of a famous architecture called [AlexNet](https://en.wikipedia.org/wiki/AlexNet) (we'll talk more about \"famous architectures\" next Chapter):\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/alexnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTmP3RwogwwM"
   },
   "source": [
    "You actually know what all of the above means now! But, deep learning and CNN architecture remains very much an art. Here is my general recipe book (based on experience, common practice, and popular pre-made architectures - more on those next chapter).\n",
    "\n",
    "Typical ingredients (in order):\n",
    "- Convolution layer(s): `torch.nn.Conv2d`\n",
    "- Activation function: `torch.nn.ReLU`, `torch.nn.Sigmoid`, `torch.nn.Softplus`, etc.\n",
    "- (optional) Batch normalization: `torch.nn.BatchNorm2d` (more on that next Chapter)\n",
    "- (optional) Pooling: `torch.nn.MaxPool2d`\n",
    "- (optional) Drop out: `torch.nn.Dropout`\n",
    "- Flatten: `torch.nn.Flatten`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5zsRcSMgwwN"
   },
   "source": [
    "## 4. CNN vs Fully Connected NN\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bk3-iXrogwwN"
   },
   "source": [
    "As an example of the parameter savings introduced when using CNNs with structured data, let's compare the Bitmoji classifier from last chapter, with an equivalent CNN version.\n",
    "\n",
    "We'll replace all linear layers with convolutional layers with 3 kernels of size (3, 3) and will assume an image size of 128 x 128:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sr8_2KwsgwwN"
   },
   "outputs": [],
   "source": [
    "def linear_block(input_size, output_size):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(input_size, output_size),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.main = torch.nn.Sequential(\n",
    "            linear_block(input_size, 256),\n",
    "            linear_block(256, 128),\n",
    "            linear_block(128, 64),\n",
    "            linear_block(64, 16),\n",
    "            torch.nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meGjREAbgwwN"
   },
   "outputs": [],
   "source": [
    "model = NN(input_size=128 * 128)\n",
    "summary(model, (128 * 128,));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gZFClyrmh9s"
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv_block(input_channels, output_channels):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(input_channels, output_channels, (3, 3), padding=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "  def __init__(self, input_channels):\n",
    "      super().__init__()\n",
    "      self.main = torch.nn.Sequential(\n",
    "          conv_block(input_channels, 3),\n",
    "          conv_block(3, 3),\n",
    "          conv_block(3, 3),\n",
    "          conv_block(3, 3),\n",
    "          conv_block(3, 3),\n",
    "          torch.nn.Flatten(),\n",
    "          torch.nn.Linear(49152, 1)\n",
    "      )\n",
    "\n",
    "  def forward(self, x):\n",
    "      out = self.main(x)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yox0D3V_gwwN"
   },
   "outputs": [],
   "source": [
    "model = CNN(input_channels=1)\n",
    "summary(model, (1, 128, 128));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
